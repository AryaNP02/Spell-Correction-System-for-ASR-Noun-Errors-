{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6QV1912xb8tt86TdWuGf+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fbb1d248f8a549e9ab46f850c96adc87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80a67ec996b24f389ce5d7324a2cc564","IPY_MODEL_c6cdadaef52d445e878416f6bb6af679","IPY_MODEL_d22b2c1cf69f4837abb8f6d6ec7e29c2"],"layout":"IPY_MODEL_a2744625368d4e679354d20c6587afbf"}},"80a67ec996b24f389ce5d7324a2cc564":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a35982a99cdf480bb2fde4206ad1b190","placeholder":"​","style":"IPY_MODEL_c310cfc5f81e4ad8955b6a8a0e25d8fe","value":"100%"}},"c6cdadaef52d445e878416f6bb6af679":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d3065bacb964d8dbf951d2f33ead42b","max":1497,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9761975554574682bc57b54966dd285e","value":1497}},"d22b2c1cf69f4837abb8f6d6ec7e29c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07c62a8e7ee1418282d87ca640ce3dff","placeholder":"​","style":"IPY_MODEL_13f768ecc5b8434fb5095c53fb99bc1d","value":" 1497/1497 [00:01&lt;00:00, 786.43it/s]"}},"a2744625368d4e679354d20c6587afbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a35982a99cdf480bb2fde4206ad1b190":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c310cfc5f81e4ad8955b6a8a0e25d8fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d3065bacb964d8dbf951d2f33ead42b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9761975554574682bc57b54966dd285e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07c62a8e7ee1418282d87ca640ce3dff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13f768ecc5b8434fb5095c53fb99bc1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Imports and Installs"],"metadata":{"id":"azsGFChbi03V"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYdeOD1EhmYv","executionInfo":{"status":"ok","timestamp":1765508999768,"user_tz":480,"elapsed":88674,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"7b082923-bff7-4536-d1e7-cb9884b18c92"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Libraries imported successfully.\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n","Collecting jiwer\n","  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n","Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n","Collecting rapidfuzz>=3.9.7 (from jiwer)\n","  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n","Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n","Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n","Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n","Looking in indexes: https://download.pytorch.org/whl/cu121\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import os\n","import difflib\n","import nltk\n","nltk.download('punkt_tab')\n","from collections import Counter, defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm.notebook import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Downloading necessary NLTK data\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","try:\n","    nltk.data.find('taggers/averaged_perceptron_tagger')\n","except LookupError:\n","    nltk.download('averaged_perceptron_tagger')\n","try:\n","    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n","except LookupError:\n","    nltk.download('averaged_perceptron_tagger_eng')\n","try:\n","    nltk.data.find('corpra/wordnet')\n","except LookupError:\n","    nltk.download('wordnet')\n","\n","\n","!pip install pandas transformers datasets accelerate jiwer scikit-learn sentencepiece\n","!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import (\n","    T5Tokenizer,\n","    T5ForConditionalGeneration,\n","    Seq2SeqTrainer,\n","    Seq2SeqTrainingArguments,\n","    DataCollatorForSeq2Seq\n",")\n","from datasets import Dataset\n","from jiwer import wer\n","\n","from google.colab import drive\n","\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","\n","print(\"All Libraries imported successfully.\")"]},{"cell_type":"markdown","source":["# 1. Load and Clean Dataset"],"metadata":{"id":"Otcniu7si8x3"}},{"cell_type":"code","source":["class DataLoader:\n","    def __init__(self, file_path):\n","        self.file_path = file_path\n","        self.df = None\n","        self.train_df = None\n","        self.val_df = None\n","        self.test_df = None\n","\n","    def load_data(self):\n","        print(f\"Loading data from {self.file_path}...\")\n","        try:\n","            self.df = pd.read_excel(self.file_path)\n","            self.df.columns = ['correct_sentence', 'incorrect_sentence']\n","            print(f\"Data loaded. Shape: {self.df.shape}\")\n","            return self.df\n","        except Exception as e:\n","            print(f\"Error loading data: {e}\")\n","            raise\n","\n","    def clean_text(self, text):\n","      if not isinstance(text, str):\n","        return str(text)\n","\n","      # 1. Removing leading/trailing quotes\n","      text = text.strip('\"')\n","\n","      # 2. Normalizing distinct punctuation\n","      text = text.replace(\"’\", \"'\")\n","\n","      # Only removing these at the end as they are stop-char.\n","      # In between the sentence, they are actually part of mispelled so need to\n","      # be conserved\n","      text = text.strip(\",\")\n","      text = text.strip(\"?\")\n","      text = text.strip(\".\")\n","\n","      # 3. Removing bullet points and other non-essential symbols\n","      text = text.strip(\"•\")\n","\n","      # Removing leading-trailing space\n","      text = text.strip()\n","\n","      # 4. Collapsing multiple spaces into one\n","      text = re.sub(r'\\s+', ' ', text).strip()\n","\n","      return text\n","\n","    def preprocess(self):\n","        if self.df is None:\n","            self.load_data()\n","\n","        print(\"Preprocessing data...\")\n","        self.df['correct_sentence'] = self.df['correct_sentence'].apply(self.clean_text)\n","        self.df['incorrect_sentence'] = self.df['incorrect_sentence'].apply(self.clean_text)\n","\n","        initial_count = len(self.df)\n","        self.df.drop_duplicates(inplace=True)\n","        print(f\"Removed {initial_count - len(self.df)} duplicates.\")\n","        return self.df\n","\n","    def split_data(self, test_size=0.15, val_size=0.15, random_state=42):\n","        if self.df is None:\n","            self.preprocess()\n","\n","        print(\"Splitting data...\")\n","        remaining_df, self.test_df = train_test_split(\n","            self.df, test_size=test_size, random_state=random_state\n","        )\n","\n","        relative_val_size = val_size / (1 - test_size)\n","        self.train_df, self.val_df = train_test_split(\n","            remaining_df, test_size=relative_val_size, random_state=random_state\n","        )\n","\n","        print(f\"Train size: {len(self.train_df)}\")\n","        print(f\"Validation size: {len(self.val_df)}\")\n","        print(f\"Test size: {len(self.test_df)}\")\n","\n","        return self.train_df, self.val_df, self.test_df"],"metadata":{"id":"DmoOE2JjiEJy","executionInfo":{"status":"ok","timestamp":1765509203506,"user_tz":480,"elapsed":42,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","\n","# Read Dataset\n","xlsx_path = \"/content/drive/MyDrive/NLP Assignment Submission/Spell_Correction_for_ASR_Noun_Enhancement_assignment_dataset.xlsx\"\n","if os.path.exists(xlsx_path):\n","    loader = DataLoader(xlsx_path)\n","    train_df, val_df, test_df = loader.split_data()\n","else:\n","    print(\"Dataset not found. Please ensure the .xlsx file is present.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CyklxZWsiISs","executionInfo":{"status":"ok","timestamp":1765509206894,"user_tz":480,"elapsed":2281,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"b407ba13-eef1-4a0e-b4a8-659513c53447"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loading data from /content/drive/MyDrive/NLP Assignment Submission/Spell_Correction_for_ASR_Noun_Enhancement_assignment_dataset.xlsx...\n","Data loaded. Shape: (10000, 2)\n","Preprocessing data...\n","Removed 23 duplicates.\n","Splitting data...\n","Train size: 6983\n","Val size: 1497\n","Test size: 1497\n"]}]},{"cell_type":"markdown","source":["## 1.1 Exploratory Data Analysis (EDA)"],"metadata":{"id":"zG1r7PppjBYt"}},{"cell_type":"code","source":["\n","def run_eda(df, set_name=\"Training\"):\n","    print(f\"--- EDA for {set_name} Set ---\")\n","    correct_lens = df['correct_sentence'].apply(lambda x: len(str(x).split()))\n","    incorrect_lens = df['incorrect_sentence'].apply(lambda x: len(str(x).split()))\n","\n","    print(f\"Average Correct Sentence Length: {correct_lens.mean():.2f} words\")\n","    print(f\"Average Incorrect Sentence Length: {incorrect_lens.mean():.2f} words\")\n","\n","    # Analysing Vocabulary\n","    all_text = \" \".join(df['correct_sentence'].astype(str))\n","    vocab = set(all_text.split())\n","    print(f\"Vocabulary Size: {len(vocab)} unique words\")\n","\n","    # Finding Common words\n","    word_counts = Counter(all_text.split())\n","    print(\"Top 10 most common words:\")\n","    print(word_counts.most_common(100))\n","    print(\"\\n\")\n","\n","if 'train_df' in locals():\n","    run_eda(train_df, \"Training\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOVJuJHWjLvM","executionInfo":{"status":"ok","timestamp":1765509219157,"user_tz":480,"elapsed":11,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"991d6e57-59e1-45d7-819a-785aac20c0d4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--- EDA for Training Set ---\n","Average Correct Sentence Length: 12.74 words\n","Average Incorrect Sentence Length: 13.03 words\n","Vocabulary Size: 9957 unique words\n","Top 10 most common words:\n","[('to', 3968), ('is', 3753), ('for', 2714), ('a', 2374), ('your', 1987), ('medication', 1669), ('and', 1663), ('of', 1655), ('the', 1569), ('used', 1431), ('prescribed', 1294), ('in', 1258), ('treat', 1162), ('commonly', 1093), ('healthcare', 869), ('with', 756), ('as', 755), ('follow', 724), ('taking', 673), ('by', 666), ('symptoms', 654), ('help', 638), ('infections', 617), ('provider', 593), ('you', 578), ('when', 572), ('The', 563), ('instructions', 559), ('treating', 539), ('conditions', 492), ('sure', 471), ('its', 470), ('pain', 470), ('important', 448), ('effective', 434), ('relief', 431), ('Make', 431), ('an', 415), ('bacterial', 393), ('effectiveness', 390), ('blood', 388), ('dosage', 385), ('can', 367), ('be', 364), ('take', 357), ('results', 355), ('known', 351), ('from', 336), ('Have', 332), ('It', 325), ('health', 319), ('certain', 316), ('optimal', 307), ('doctor', 298), ('manage', 293), ('medicine', 293), ('carefully', 276), ('treatment', 268), ('pressure', 266), ('various', 263), ('popular', 261), ('effectively', 260), ('directed', 259), ('may', 255), ('effects', 253), ('Remember', 250), ('inflammation', 245), ('side', 230), ('should', 229), ('antibiotic', 222), ('recommended', 222), ('managing', 217), ('before', 217), ('respiratory', 211), (\"doctor's\", 208), ('high', 207), ('consult', 205), (\"provider's\", 201), ('using', 201), ('that', 195), ('tried', 186), ('potential', 182), ('any', 180), ('powerful', 178), ('prescription', 175), ('levels', 173), ('starting', 169), ('often', 167), ('skin', 165), ('patients', 163), ('alleviate', 163), ('acid', 162), ('such', 160), ('their', 157), ('ensure', 155), ('contains', 153), ('helps', 149), ('taken', 147), ('improve', 145), ('reduce', 145)]\n","\n","\n"]}]},{"cell_type":"markdown","source":["# 2. Error Analysis"],"metadata":{"id":"oc9qdMjUjRRm"}},{"cell_type":"markdown","source":["## 2.2 Preprocessing (NER & POS Tagging)"],"metadata":{"id":"CkHf38vmjTeU"}},{"cell_type":"code","source":["def preprocess_pos_ner(df):\n","    print(\"Running POS Tagging and NER (Noun Extraction)...\")\n","    def get_nouns(text):\n","        tokens = nltk.word_tokenize(str(text))\n","        tags = nltk.pos_tag(tokens)\n","        nouns = [word for word, pos in tags if pos.startswith('NN')]\n","        return nouns\n","\n","    df['correct_nouns'] = df['correct_sentence'].apply(get_nouns)\n","    print(\"Nouns extracted.\")\n","    return df\n","\n","if 'train_df' in locals():\n","    train_df = preprocess_pos_ner(train_df)\n","    if 'val_df' in locals(): val_df = preprocess_pos_ner(val_df)\n","    test_df = preprocess_pos_ner(test_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qiLpGVtjVYD","executionInfo":{"status":"ok","timestamp":1765509272481,"user_tz":480,"elapsed":13089,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"e36ff115-cc01-4005-cabd-c6efd82d50f1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Running POS Tagging and NER (Noun Extraction)...\n","Nouns extracted.\n","Running POS Tagging and NER (Noun Extraction)...\n","Nouns extracted.\n","Running POS Tagging and NER (Noun Extraction)...\n","Nouns extracted.\n"]}]},{"cell_type":"markdown","source":["2.2 Error Analysis"],"metadata":{"id":"y1UrWtlqjYV1"}},{"cell_type":"code","source":["\n","class ErrorAnalyzer:\n","    def __init__(self, df):\n","        self.df = df\n","        self.error_pairs = []\n","        self.detailed_df = pd.DataFrame()\n","\n","    def get_diff_ops(self, correct_sent, incorrect_sent):\n","        c_words = correct_sent.split()\n","        i_words = incorrect_sent.split()\n","        matcher = difflib.SequenceMatcher(None, [i.lower() for i in c_words], [i.lower() for i in i_words])\n","        return matcher.get_opcodes(), c_words, i_words\n","\n","    # Identifying the type of missmatch\n","    def categorize_error(self, correct, incorrect):\n","        if not correct or not incorrect: return \"Insertion/Deletion\"\n","\n","        ratio = difflib.SequenceMatcher(None, correct, incorrect).ratio()\n","        if ratio > 0.8:\n","            return \"Character-level (Likely Typo)\"\n","        elif any(char.isdigit() for char in incorrect):\n","            return \"Formatting/Number\"\n","        else:\n","             # Checking phonetic similarity approximation\n","             if len(correct) == len(incorrect):\n","                 return \"Phonetic/Substitution\"\n","        return \"Word-level/Other\"\n","\n","    def extract_errors(self):\n","        print(\"Extracting errors...\")\n","        detailed_errors = []\n","\n","        for idx, row in self.df.iterrows():\n","            correct = str(row['correct_sentence'])\n","            incorrect = str(row['incorrect_sentence'])\n","            nouns = set(row['correct_nouns']) if 'correct_nouns' in row else set()\n","\n","            opcodes, c_words, i_words = self.get_diff_ops(correct, incorrect)\n","            for tag, i1, i2, j1, j2 in opcodes:\n","                if tag == 'replace':\n","                    c_segment = \" \".join(c_words[i1:i2])\n","                    i_segment = \" \".join(i_words[j1:j2])\n","\n","                    category = self.categorize_error(c_segment, i_segment)\n","\n","                    is_noun_error = all(word in nouns for word in c_segment.split())\n","\n","                    if not is_noun_error:\n","                      continue\n","\n","                    # for noun word eg: effect,\n","                    if (c_segment.strip(\",\") == i_segment.strip(\",\")):\n","                      continue\n","\n","                    self.error_pairs.append((c_segment, i_segment))\n","                    detailed_errors.append({\n","                        'correct': c_segment,\n","                        'incorrect': i_segment,\n","                        'type': category,\n","                        'is_noun_error': is_noun_error\n","                    })\n","\n","        self.detailed_df = pd.DataFrame(detailed_errors)\n","        return self.error_pairs\n","\n","    def get_common_errors(self, n=20):\n","        return Counter(self.error_pairs).most_common(n)\n","\n","    def get_all_errors(self):\n","        return Counter(self.error_pairs).most_common()\n"],"metadata":{"id":"vNFHoiYjjaww","executionInfo":{"status":"ok","timestamp":1765509289742,"user_tz":480,"elapsed":10,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["if 'train_df' in locals():\n","    analyzer = ErrorAnalyzer(train_df)\n","    analyzer.extract_errors()\n","    print(\"\\nStats of Common Errors:\")\n","\n","    diff_words_list = []\n","    diff_words_list_map = {}\n","    for (correct, incorrect), count in analyzer.get_all_errors():\n","      diff_words_list.append((correct, incorrect, count))\n","\n","    for (correct, incorrect), count in analyzer.get_common_errors(10):\n","        print(f\"'{correct}' -> '{incorrect}' ({count} times)\")\n","\n","    if not analyzer.detailed_df.empty:\n","        print(\"\\nError Category Distribution:\")\n","        print(analyzer.detailed_df['type'].value_counts())\n","        print(\"\\nNoun-related Errors:\")\n","        print(analyzer.detailed_df['is_noun_error'].value_counts())\n","        print(\"Total Mis-spelled words: \", len(diff_words_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0liGCV4kjeGv","executionInfo":{"status":"ok","timestamp":1765509295716,"user_tz":480,"elapsed":1933,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"be277941-bb41-46b8-d099-6ae28665d3b1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting errors...\n","\n","Stats of Common Errors Found in dataset:\n","'healthcare' -> 'health care' (473 times)\n","'sulphate' -> 'sulfate' (16 times)\n","'vitamin' -> 'vitam in' (11 times)\n","'maleate' -> 'malate' (6 times)\n","'Levocetirizine' -> 'Levosterazine' (5 times)\n","'sulfate' -> 'sulphate' (5 times)\n","'skincare' -> 'skin care' (5 times)\n","'Ofloxacin' -> 'Oflixacin' (4 times)\n","'healthcare provider' -> 'health care provider,' (4 times)\n","'betamethasone' -> 'beta-methasone' (4 times)\n","\n","Error Category Distribution:\n","type\n","Word-level/Other                 2637\n","Phonetic/Substitution            2111\n","Character-level (Likely Typo)    1858\n","Formatting/Number                 103\n","Name: count, dtype: int64\n","\n","Noun-related Errors:\n","is_noun_error\n","True    6709\n","Name: count, dtype: int64\n","Total Mis-spelled words:  6032\n"]}]},{"cell_type":"markdown","source":["## 3. Model Development"],"metadata":{"id":"Ky4octeOjhJB"}},{"cell_type":"markdown","source":["## 3.1 Baseline Model: Levenshtein distance & N-gram language models\n","Using Edit distance algorithms (Levenshtein distance) to find correct words candidate for potentially mis-spelled word and using N-gram language models to pick the best match from the candidates"],"metadata":{"id":"rRbw8CAqjjFI"}},{"cell_type":"code","source":["class BaselineSpellCorrector:\n","    def __init__(self):\n","        # To save words in lower case for NLP\n","        self.vocab = set()\n","        # To save the original word case. There is possibilty to get overrided\n","        # as word with different case has same lower case. But should help for\n","        # medical names\n","        self.vocab_to_original = {}\n","        self.unigram_counts = Counter()\n","        self.bigram_counts = defaultdict(int)\n","\n","    # We want to lower all words for NLP process but for final result, we want\n","    # to preserve the original case for reason such as preserving medicine names\n","    # same as ground truth\n","    def tokenize(self, text, to_lower=True):\n","        \"\"\"\n","        Simple tokenizer that separates words from punctuation.\n","        This ensures 'tablet.' and 'tablet' are treated as the same word 'tablet' + '.'\n","        \"\"\"\n","        #return re.findall(r'\\w+|[^\\w\\s]', str(text).lower())\n","        if to_lower:\n","          return re.findall(r'[a-zA-Z0-9]+(?:-[a-zA-Z0-9]+)*', str(text).lower())\n","        else:\n","          return re.findall(r'[a-zA-Z0-9]+(?:-[a-zA-Z0-9]+)*', str(text))\n","\n","\n","    def train(self, sentences):\n","        \"\"\"\n","        Build the Dictionary (vocab) and Language Model (N-grams) from correct text.\n","        \"\"\"\n","        print(\"Training Baseline Model...\")\n","        for sentence in sentences:\n","            tokens = self.tokenize(sentence, to_lower=True)\n","            tokens_not_lower = self.tokenize(sentence, to_lower=False)\n","\n","            # 1. Updating Dictionary & Frequency\n","            for ii in range(len(tokens)):\n","              self.vocab.add(tokens[ii])\n","              self.vocab_to_original[tokens[ii]] = tokens_not_lower[ii]\n","\n","            self.unigram_counts.update(tokens)\n","\n","            # 2. Updating Context (Bigrams)\n","            for i in range(len(tokens) - 1):\n","                self.bigram_counts[(tokens[i], tokens[i+1])] += 1\n","\n","        print(f\"Training & bi-gram Complete. Vocabulary Size: {len(self.vocab)}\")\n","\n","    def is_edit_score_close(self, score1, score2):\n","        if score2 == 0:\n","          if score1 != 0:\n","              return False\n","          return True\n","        return ((score1-score2)/score1)*100 <= 0.1\n","\n","    def get_candidates(self, word):\n","        \"\"\"\n","        Find correction candidates using Edit Distance.\n","        \"\"\"\n","        # If word is in vocabulary, it's the only candidate\n","        if word.lower() in self.vocab:\n","            return [word]\n","\n","        # Find closest matches in vocab\n","        # n=3: Top 3 matches\n","        # cutoff=0.6: Matches must be at least 60% similar\n","        matches = difflib.get_close_matches(word.lower(), self.vocab, n=3, cutoff=0.6)\n","\n","        # If no similar words found, then return original\n","        if not matches:\n","            return [word]\n","\n","        candidate_score = [\n","            (m, difflib.SequenceMatcher(None, word, m).ratio())\n","            for m in matches\n","        ]\n","\n","        final_matched = [self.vocab_to_original[matches[0]]]\n","\n","        # Only those cadidate whose edit score is very close to the best one are\n","        # to be considered as candidates\n","        if len(matches) > 1 and self.is_edit_score_close(candidate_score[0][1], candidate_score[1][1]):\n","            final_matched.append(self.vocab_to_original[matches[1]])\n","\n","        if len(matches) > 2 and self.is_edit_score_close(candidate_score[0][1], candidate_score[2][1]):\n","            final_matched.append(self.vocab_to_original[matches[2]])\n","\n","        return final_matched\n","\n","    def correct_sentence(self, sentence):\n","        \"\"\"\n","        Correct a sentence using the trained model.\n","        \"\"\"\n","        tokens_orig = self.tokenize(sentence, to_lower=False)\n","        tokens = self.tokenize(sentence)\n","        corrected_tokens = []\n","\n","        for i, word in enumerate(tokens):\n","\n","            candidates = self.get_candidates(word)\n","\n","            if len(candidates) == 1:\n","                best_word = candidates[0]\n","            else:\n","                # Ranking candidates by Context score\n","                best_word = candidates[0]\n","                best_score = -1\n","\n","                # Looking at the previous corrected word for context\n","                prev_token = corrected_tokens[-1] if corrected_tokens else \"START\"\n","\n","                for cand in candidates:\n","                    # Score = (BigramProb * 10) + UnigramProb\n","                    # Weight Bigrams higher than raw frequency\n","                    bigram_score = self.bigram_counts[(prev_token, cand)]\n","                    unigram_score = self.unigram_counts[cand]\n","\n","                    score = (bigram_score * 10) + unigram_score\n","\n","                    if score > best_score:\n","                        best_score = score\n","                        best_word = cand\n","\n","            corrected_tokens.append(best_word)\n","\n","        # Reconstructing sentence\n","        return \" \".join(corrected_tokens)\n","\n","# 1. Initialize and Train\n","baseline_model = BaselineSpellCorrector()\n","baseline_model.train(train_df['correct_sentence'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpYdhikDjlKC","executionInfo":{"status":"ok","timestamp":1765509325594,"user_tz":480,"elapsed":134,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"8c7dccc3-4116-42a7-f0d4-f0f121d4d634"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Baseline Model...\n","Training & bi-gram Complete. Vocabulary Size: 9147\n"]}]},{"cell_type":"markdown","source":["## 3.1.1 Prediction on Test Dataset"],"metadata":{"id":"Hj0u2K7sjoff"}},{"cell_type":"markdown","source":["#### Prediction on a Sample"],"metadata":{"id":"AerQSfQGjq_b"}},{"cell_type":"code","source":["\n","print(\"\\n--- Baseline Model Predictions (Sample) ---\")\n","for idx, row in test_df.head(5).iterrows():\n","    input_text = row['incorrect_sentence']\n","    ground_truth = row['correct_sentence']\n","    prediction = baseline_model.correct_sentence(input_text)\n","\n","    print(f\"Input:    {input_text}\")\n","    print(f\"Pred:     {prediction}\")\n","    print(f\"Actual:   {ground_truth}\")\n","    print(\"-\" * 30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYV4mLdNjsQx","executionInfo":{"status":"ok","timestamp":1765509353063,"user_tz":480,"elapsed":272,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"c349a826-ab82-45a3-ca48-1437d0c2c955"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Baseline Model Predictions (Sample) ---\n","Input:    The effectiveness of AXIS-P in managing symptoms has been well documented\n","Pred:     the effectiveness of AXCES-P in managing symptoms has been well documented\n","Actual:   The effectiveness of AXSES-P in managing symptoms has been well-documented\n","------------------------------\n","Input:    Dossalopin may cause drowsiness and should be taken as prescribed by a healthcare professional\n","Pred:     Dosulepin may cause drowsiness and should be taken as prescribed by a healthcare professional\n","Actual:   Dosulepin may cause drowsiness and should be taken as prescribed by a healthcare professional\n","------------------------------\n","Input:    Prochloropyrazine mesylate is commonly used to treat nausea and vomiting\n","Pred:     Prochlorperazine mesylate is commonly used to treat nausea and vomiting\n","Actual:   Prochlorperazine mesylate is commonly used to treat nausea and vomiting\n","------------------------------\n","Input:    nettle miss insulphate should be used exactly as prescribed by your health care provider to ensure effectiveness in treating the infection\n","Pred:     betle miss Sulphate should be used exactly as prescribed by your health care provider to ensure effectiveness in treating the infection\n","Actual:   Netilmicin sulfate should be used exactly as prescribed by your healthcare provider to ensure effectiveness in treating the infection\n","------------------------------\n","Input:    One recommended dosage of calcium carbonate is taking one SG cap daily for optimal results\n","Pred:     one recommended dosage of calcium carbonate is taking one s cap daily for optimal results\n","Actual:   One recommended dosage of calcium carbonate is taking 1 sg-cap daily for optimal results\n","------------------------------\n"]}]},{"cell_type":"markdown","source":["#### Prediction on a full test data"],"metadata":{"id":"4z7eLZKCjvG_"}},{"cell_type":"code","source":["if 'test_df' in locals():\n","    print(\"Predicting with Baseline...\")\n","    test_df['baseline_prediction'] = test_df['incorrect_sentence'].apply(baseline_model.correct_sentence)\n","    print(\"Baseline predictions complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJecyz4Ojwm9","executionInfo":{"status":"ok","timestamp":1765509433076,"user_tz":480,"elapsed":62430,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"48207b10-4eac-4153-bc11-7d53f52146b6"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting with Baseline...\n","Baseline predictions complete.\n"]}]},{"cell_type":"markdown","source":["# 4.Evaluation"],"metadata":{"id":"5WXuY2f5j0xP"}},{"cell_type":"markdown","source":["## 4.1 Baseline Model\n","#### Word-Level Accuracy, WER, CER, BLEU and Mean Noun Recall"],"metadata":{"id":"5z0j8s53j3K3"}},{"cell_type":"code","source":["def calculate_word_level_accuracy():\n","  print(\"Calculating Accuracy on Test Set...\")\n","  total_words = 0\n","  correct_words = 0\n","  for idx, row in test_df.iterrows():\n","      # Stripping punctuation for the metric calculation to be fair\n","      pred_words = baseline_model.correct_sentence(row['incorrect_sentence']).split()\n","      target_words = str(row['correct_sentence']).lower().split()\n","\n","      # Comparing word-for-word\n","      length = min(len(pred_words), len(target_words))\n","      for i in range(length):\n","          if pred_words[i] == target_words[i]:\n","              correct_words += 1\n","\n","      total_words += len(target_words)\n","\n","  accuracy = correct_words / total_words if total_words > 0 else 0\n","  print(f\"\\n Word-Level Accuracy: {accuracy:.2%}\")\n","\n","def calculate_wer(reference, hypothesis):\n","    ref_words = reference.split()\n","    hyp_words = hypothesis.split()\n","    d = np.zeros((len(ref_words)+1, len(hyp_words)+1))\n","    for i in range(len(ref_words)+1): d[i,0] = i\n","    for j in range(len(hyp_words)+1): d[0,j] = j\n","    for i in range(1, len(ref_words)+1):\n","        for j in range(1, len(hyp_words)+1):\n","            cost = 0 if ref_words[i-1] == hyp_words[j-1] else 1\n","            d[i,j] = min(d[i-1,j]+1, d[i,j-1]+1, d[i-1,j-1]+cost)\n","    return d[-1,-1] / len(ref_words) if len(ref_words)>0 else 0\n","\n","def evaluate_detailed(df, pred_col, target_col='correct_sentence'):\n","    wer_scores = []\n","    cer_scores = []\n","    bleu_scores = []\n","    noun_scores = []\n","\n","    print(f\"\\n--- Detailed Evaluation for {pred_col} ---\")\n","\n","    for _, row in tqdm(df.iterrows(), total=len(df)):\n","        ref = str(row[target_col])\n","        hyp = str(row[pred_col])\n","\n","        # WER\n","        wer_scores.append(calculate_wer(ref, hyp))\n","\n","        # CER (Approximate using SequenceMatcher ratio)\n","        cer_scores.append(1 - difflib.SequenceMatcher(None, ref, hyp).ratio())\n","\n","        # BLEU\n","        try:\n","            bleu = sentence_bleu([ref.split()], hyp.split(), weights=(1,0,0,0)) # BLEU-1\n","        except:\n","            bleu = 0\n","        bleu_scores.append(bleu)\n","\n","        # Noun Recall\n","        if 'correct_nouns' in row and row['correct_nouns']:\n","            nouns = row['correct_nouns']\n","            hyp_words = set(hyp.split())\n","            matches = sum(1 for n in nouns if n in hyp_words)\n","            noun_scores.append(matches / len(nouns))\n","        else:\n","            noun_scores.append(1.0) # No nouns to miss\n","\n","    print(f\"Mean WER: {np.mean(wer_scores):.4f} (Lower is better)\")\n","    print(f\"Mean CER: {np.mean(cer_scores):.4f} (Lower is better)\")\n","    print(f\"Mean BLEU-1: {np.mean(bleu_scores):.4f} (Higher is better)\")\n","    print(f\"Mean Noun Recall: {np.mean(noun_scores):.4f} (Higher is better)\")\n","\n","if 'baseline_prediction' in test_df.columns:\n","    calculate_word_level_accuracy()\n","    evaluate_detailed(test_df, 'baseline_prediction')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210,"referenced_widgets":["fbb1d248f8a549e9ab46f850c96adc87","80a67ec996b24f389ce5d7324a2cc564","c6cdadaef52d445e878416f6bb6af679","d22b2c1cf69f4837abb8f6d6ec7e29c2","a2744625368d4e679354d20c6587afbf","a35982a99cdf480bb2fde4206ad1b190","c310cfc5f81e4ad8955b6a8a0e25d8fe","0d3065bacb964d8dbf951d2f33ead42b","9761975554574682bc57b54966dd285e","07c62a8e7ee1418282d87ca640ce3dff","13f768ecc5b8434fb5095c53fb99bc1d"]},"id":"1KVWcOhVj48O","executionInfo":{"status":"ok","timestamp":1765509556032,"user_tz":480,"elapsed":59967,"user":{"displayName":"Aryan Prajapati","userId":"07150843219375122767"}},"outputId":"d4bd4f2d-4ed4-4a1c-e21d-739af258182a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Accuracy on Test Set...\n","\n","Baseline Word-Level Accuracy: 62.42%\n","\n","--- Detailed Evaluation for baseline_prediction ---\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1497 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbb1d248f8a549e9ab46f850c96adc87"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mean WER: 0.1966 (Lower is better)\n","Mean CER: 0.0781 (Lower is better)\n","Mean BLEU-1: 0.8139 (Higher is better)\n","Mean Noun Recall: 0.6921 (Higher is better)\n"]}]}]}
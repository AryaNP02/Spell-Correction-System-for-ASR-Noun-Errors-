{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azsGFChbi03V"
   },
   "source": [
    "# Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88674,
     "status": "ok",
     "timestamp": 1765508999768,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "VYdeOD1EhmYv",
    "outputId": "7b082923-bff7-4536-d1e7-cb9884b18c92"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import difflib\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Downloading necessary NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "try:\n",
    "    nltk.data.find('corpra/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "\n",
    "!pip install pandas transformers datasets accelerate jiwer scikit-learn sentencepiece\n",
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "from jiwer import wer\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "print(\"All Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otcniu7si8x3"
   },
   "source": [
    "# 1. Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1765509203506,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "DmoOE2JjiEJy"
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.test_df = None\n",
    "\n",
    "    def load_data(self):\n",
    "        print(f\"Loading data from {self.file_path}...\")\n",
    "        try:\n",
    "            self.df = pd.read_excel(self.file_path)\n",
    "            self.df.columns = ['correct_sentence', 'incorrect_sentence']\n",
    "            print(f\"Data loaded. Shape: {self.df.shape}\")\n",
    "            return self.df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_text(self, text):\n",
    "      if not isinstance(text, str):\n",
    "        return str(text)\n",
    "\n",
    "      # 1. Removing leading/trailing quotes\n",
    "      text = text.strip('\"')\n",
    "\n",
    "      # 2. Normalizing distinct punctuation\n",
    "      text = text.replace(\"’\", \"'\")\n",
    "\n",
    "      # Only removing these at the end as they are stop-char.\n",
    "      # In between the sentence, they are actually part of mispelled so need to\n",
    "      # be conserved\n",
    "      text = text.strip(\",\")\n",
    "      text = text.strip(\"?\")\n",
    "      text = text.strip(\".\")\n",
    "\n",
    "      # 3. Removing bullet points and other non-essential symbols\n",
    "      text = text.strip(\"•\")\n",
    "\n",
    "      # Removing leading-trailing space\n",
    "      text = text.strip()\n",
    "\n",
    "      # 4. Collapsing multiple spaces into one\n",
    "      text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "      return text\n",
    "\n",
    "    def preprocess(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        print(\"Preprocessing data...\")\n",
    "        self.df['correct_sentence'] = self.df['correct_sentence'].apply(self.clean_text)\n",
    "        self.df['incorrect_sentence'] = self.df['incorrect_sentence'].apply(self.clean_text)\n",
    "\n",
    "        initial_count = len(self.df)\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "        print(f\"Removed {initial_count - len(self.df)} duplicates.\")\n",
    "        return self.df\n",
    "\n",
    "    def split_data(self, test_size=0.15, val_size=0.15, random_state=42):\n",
    "        if self.df is None:\n",
    "            self.preprocess()\n",
    "\n",
    "        print(\"Splitting data...\")\n",
    "        remaining_df, self.test_df = train_test_split(\n",
    "            self.df, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        relative_val_size = val_size / (1 - test_size)\n",
    "        self.train_df, self.val_df = train_test_split(\n",
    "            remaining_df, test_size=relative_val_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        print(f\"Train size: {len(self.train_df)}\")\n",
    "        print(f\"Validation size: {len(self.val_df)}\")\n",
    "        print(f\"Test size: {len(self.test_df)}\")\n",
    "\n",
    "        return self.train_df, self.val_df, self.test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2281,
     "status": "ok",
     "timestamp": 1765509206894,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "CyklxZWsiISs",
    "outputId": "b407ba13-eef1-4a0e-b4a8-659513c53447"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "# Read Dataset\n",
    "xlsx_path = \"/content/drive/MyDrive/NLP Assignment Submission/Spell_Correction_for_ASR_Noun_Enhancement_assignment_dataset.xlsx\"\n",
    "if os.path.exists(xlsx_path):\n",
    "    loader = DataLoader(xlsx_path)\n",
    "    train_df, val_df, test_df = loader.split_data()\n",
    "else:\n",
    "    print(\"Dataset not found. Please ensure the .xlsx file is present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG1r7PppjBYt"
   },
   "source": [
    "## 1.1 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1765509219157,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "dOVJuJHWjLvM",
    "outputId": "991d6e57-59e1-45d7-819a-785aac20c0d4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_eda(df, set_name=\"Training\"):\n",
    "    print(f\"--- EDA for {set_name} Set ---\")\n",
    "    correct_lens = df['correct_sentence'].apply(lambda x: len(str(x).split()))\n",
    "    incorrect_lens = df['incorrect_sentence'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    print(f\"Average Correct Sentence Length: {correct_lens.mean():.2f} words\")\n",
    "    print(f\"Average Incorrect Sentence Length: {incorrect_lens.mean():.2f} words\")\n",
    "\n",
    "    # Analysing Vocabulary\n",
    "    all_text = \" \".join(df['correct_sentence'].astype(str))\n",
    "    vocab = set(all_text.split())\n",
    "    print(f\"Vocabulary Size: {len(vocab)} unique words\")\n",
    "\n",
    "    # Finding Common words\n",
    "    word_counts = Counter(all_text.split())\n",
    "    print(\"Top 10 most common words:\")\n",
    "    print(word_counts.most_common(100))\n",
    "    print(\"\\n\")\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    run_eda(train_df, \"Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc9qdMjUjRRm"
   },
   "source": [
    "# 2. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHf38vmjTeU"
   },
   "source": [
    "## 2.2 Preprocessing (NER & POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13089,
     "status": "ok",
     "timestamp": 1765509272481,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "7qiLpGVtjVYD",
    "outputId": "e36ff115-cc01-4005-cabd-c6efd82d50f1"
   },
   "outputs": [],
   "source": [
    "def preprocess_pos_ner(df):\n",
    "    print(\"Running POS Tagging and NER (Noun Extraction)...\")\n",
    "    def get_nouns(text):\n",
    "        tokens = nltk.word_tokenize(str(text))\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        nouns = [word for word, pos in tags if pos.startswith('NN')]\n",
    "        return nouns\n",
    "\n",
    "    df['correct_nouns'] = df['correct_sentence'].apply(get_nouns)\n",
    "    print(\"Nouns extracted.\")\n",
    "    return df\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    train_df = preprocess_pos_ner(train_df)\n",
    "    if 'val_df' in locals(): val_df = preprocess_pos_ner(val_df)\n",
    "    test_df = preprocess_pos_ner(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1UrWtlqjYV1"
   },
   "source": [
    "2.2 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1765509289742,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "vNFHoiYjjaww"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ErrorAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.error_pairs = []\n",
    "        self.detailed_df = pd.DataFrame()\n",
    "\n",
    "    def get_diff_ops(self, correct_sent, incorrect_sent):\n",
    "        c_words = correct_sent.split()\n",
    "        i_words = incorrect_sent.split()\n",
    "        matcher = difflib.SequenceMatcher(None, [i.lower() for i in c_words], [i.lower() for i in i_words])\n",
    "        return matcher.get_opcodes(), c_words, i_words\n",
    "\n",
    "    # Identifying the type of missmatch\n",
    "    def categorize_error(self, correct, incorrect):\n",
    "        if not correct or not incorrect: return \"Insertion/Deletion\"\n",
    "\n",
    "        ratio = difflib.SequenceMatcher(None, correct, incorrect).ratio()\n",
    "        if ratio > 0.8:\n",
    "            return \"Character-level (Likely Typo)\"\n",
    "        elif any(char.isdigit() for char in incorrect):\n",
    "            return \"Formatting/Number\"\n",
    "        else:\n",
    "             # Checking phonetic similarity approximation\n",
    "             if len(correct) == len(incorrect):\n",
    "                 return \"Phonetic/Substitution\"\n",
    "        return \"Word-level/Other\"\n",
    "\n",
    "    def extract_errors(self):\n",
    "        print(\"Extracting errors...\")\n",
    "        detailed_errors = []\n",
    "\n",
    "        for idx, row in self.df.iterrows():\n",
    "            correct = str(row['correct_sentence'])\n",
    "            incorrect = str(row['incorrect_sentence'])\n",
    "            nouns = set(row['correct_nouns']) if 'correct_nouns' in row else set()\n",
    "\n",
    "            opcodes, c_words, i_words = self.get_diff_ops(correct, incorrect)\n",
    "            for tag, i1, i2, j1, j2 in opcodes:\n",
    "                if tag == 'replace':\n",
    "                    c_segment = \" \".join(c_words[i1:i2])\n",
    "                    i_segment = \" \".join(i_words[j1:j2])\n",
    "\n",
    "                    category = self.categorize_error(c_segment, i_segment)\n",
    "\n",
    "                    is_noun_error = all(word in nouns for word in c_segment.split())\n",
    "\n",
    "                    if not is_noun_error:\n",
    "                      continue\n",
    "\n",
    "                    # for noun word eg: effect,\n",
    "                    if (c_segment.strip(\",\") == i_segment.strip(\",\")):\n",
    "                      continue\n",
    "\n",
    "                    self.error_pairs.append((c_segment, i_segment))\n",
    "                    detailed_errors.append({\n",
    "                        'correct': c_segment,\n",
    "                        'incorrect': i_segment,\n",
    "                        'type': category,\n",
    "                        'is_noun_error': is_noun_error\n",
    "                    })\n",
    "\n",
    "        self.detailed_df = pd.DataFrame(detailed_errors)\n",
    "        return self.error_pairs\n",
    "\n",
    "    def get_common_errors(self, n=20):\n",
    "        return Counter(self.error_pairs).most_common(n)\n",
    "\n",
    "    def get_all_errors(self):\n",
    "        return Counter(self.error_pairs).most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1933,
     "status": "ok",
     "timestamp": 1765509295716,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "0liGCV4kjeGv",
    "outputId": "be277941-bb41-46b8-d099-6ae28665d3b1"
   },
   "outputs": [],
   "source": [
    "if 'train_df' in locals():\n",
    "    analyzer = ErrorAnalyzer(train_df)\n",
    "    analyzer.extract_errors()\n",
    "    print(\"\\nStats of Common Errors:\")\n",
    "\n",
    "    diff_words_list = []\n",
    "    diff_words_list_map = {}\n",
    "    for (correct, incorrect), count in analyzer.get_all_errors():\n",
    "      diff_words_list.append((correct, incorrect, count))\n",
    "\n",
    "    for (correct, incorrect), count in analyzer.get_common_errors(10):\n",
    "        print(f\"'{correct}' -> '{incorrect}' ({count} times)\")\n",
    "\n",
    "    if not analyzer.detailed_df.empty:\n",
    "        print(\"\\nError Category Distribution:\")\n",
    "        print(analyzer.detailed_df['type'].value_counts())\n",
    "        print(\"\\nNoun-related Errors:\")\n",
    "        print(analyzer.detailed_df['is_noun_error'].value_counts())\n",
    "        print(\"Total Mis-spelled words: \", len(diff_words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ky4octeOjhJB"
   },
   "source": [
    "## 3. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRbw8CAqjjFI"
   },
   "source": [
    "## 3.1 Baseline Model: Levenshtein distance & N-gram language models\n",
    "Using Edit distance algorithms (Levenshtein distance) to find correct words candidate for potentially mis-spelled word and using N-gram language models to pick the best match from the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1765509325594,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "EpYdhikDjlKC",
    "outputId": "8c7dccc3-4116-42a7-f0d4-f0f121d4d634"
   },
   "outputs": [],
   "source": [
    "class BaselineSpellCorrector:\n",
    "    def __init__(self):\n",
    "        # To save words in lower case for NLP\n",
    "        self.vocab = set()\n",
    "        # To save the original word case. There is possibilty to get overrided\n",
    "        # as word with different case has same lower case. But should help for\n",
    "        # medical names\n",
    "        self.vocab_to_original = {}\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "\n",
    "    # We want to lower all words for NLP process but for final result, we want\n",
    "    # to preserve the original case for reason such as preserving medicine names\n",
    "    # same as ground truth\n",
    "    def tokenize(self, text, to_lower=True):\n",
    "        \"\"\"\n",
    "        Simple tokenizer that separates words from punctuation.\n",
    "        This ensures 'tablet.' and 'tablet' are treated as the same word 'tablet' + '.'\n",
    "        \"\"\"\n",
    "        #return re.findall(r'\\w+|[^\\w\\s]', str(text).lower())\n",
    "        if to_lower:\n",
    "          return re.findall(r'[a-zA-Z0-9]+(?:-[a-zA-Z0-9]+)*', str(text).lower())\n",
    "        else:\n",
    "          return re.findall(r'[a-zA-Z0-9]+(?:-[a-zA-Z0-9]+)*', str(text))\n",
    "\n",
    "\n",
    "    def train(self, sentences):\n",
    "        \"\"\"\n",
    "        Build the Dictionary (vocab) and Language Model (N-grams) from correct text.\n",
    "        \"\"\"\n",
    "        print(\"Training Baseline Model...\")\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence, to_lower=True)\n",
    "            tokens_not_lower = self.tokenize(sentence, to_lower=False)\n",
    "\n",
    "            # 1. Updating Dictionary & Frequency\n",
    "            for ii in range(len(tokens)):\n",
    "              self.vocab.add(tokens[ii])\n",
    "              self.vocab_to_original[tokens[ii]] = tokens_not_lower[ii]\n",
    "\n",
    "            self.unigram_counts.update(tokens)\n",
    "\n",
    "            # 2. Updating Context (Bigrams)\n",
    "            for i in range(len(tokens) - 1):\n",
    "                self.bigram_counts[(tokens[i], tokens[i+1])] += 1\n",
    "\n",
    "        print(f\"Training & bi-gram Complete. Vocabulary Size: {len(self.vocab)}\")\n",
    "\n",
    "    def is_edit_score_close(self, score1, score2):\n",
    "        if score2 == 0:\n",
    "          if score1 != 0:\n",
    "              return False\n",
    "          return True\n",
    "        return ((score1-score2)/score1)*100 <= 0.1\n",
    "\n",
    "    def get_candidates(self, word):\n",
    "        \"\"\"\n",
    "        Find correction candidates using Edit Distance.\n",
    "        \"\"\"\n",
    "        # If word is in vocabulary, it's the only candidate\n",
    "        if word.lower() in self.vocab:\n",
    "            return [word]\n",
    "\n",
    "        # Find closest matches in vocab\n",
    "        # n=3: Top 3 matches\n",
    "        # cutoff=0.6: Matches must be at least 60% similar\n",
    "        matches = difflib.get_close_matches(word.lower(), self.vocab, n=3, cutoff=0.6)\n",
    "\n",
    "        # If no similar words found, then return original\n",
    "        if not matches:\n",
    "            return [word]\n",
    "\n",
    "        candidate_score = [\n",
    "            (m, difflib.SequenceMatcher(None, word, m).ratio())\n",
    "            for m in matches\n",
    "        ]\n",
    "\n",
    "        final_matched = [self.vocab_to_original[matches[0]]]\n",
    "\n",
    "        # Only those cadidate whose edit score is very close to the best one are\n",
    "        # to be considered as candidates\n",
    "        if len(matches) > 1 and self.is_edit_score_close(candidate_score[0][1], candidate_score[1][1]):\n",
    "            final_matched.append(self.vocab_to_original[matches[1]])\n",
    "\n",
    "        if len(matches) > 2 and self.is_edit_score_close(candidate_score[0][1], candidate_score[2][1]):\n",
    "            final_matched.append(self.vocab_to_original[matches[2]])\n",
    "\n",
    "        return final_matched\n",
    "\n",
    "    def correct_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Correct a sentence using the trained model.\n",
    "        \"\"\"\n",
    "        tokens_orig = self.tokenize(sentence, to_lower=False)\n",
    "        tokens = self.tokenize(sentence)\n",
    "        corrected_tokens = []\n",
    "\n",
    "        for i, word in enumerate(tokens):\n",
    "\n",
    "            candidates = self.get_candidates(word)\n",
    "\n",
    "            if len(candidates) == 1:\n",
    "                best_word = candidates[0]\n",
    "            else:\n",
    "                # Ranking candidates by Context score\n",
    "                best_word = candidates[0]\n",
    "                best_score = -1\n",
    "\n",
    "                # Looking at the previous corrected word for context\n",
    "                prev_token = corrected_tokens[-1] if corrected_tokens else \"START\"\n",
    "\n",
    "                for cand in candidates:\n",
    "                    # Score = (BigramProb * 10) + UnigramProb\n",
    "                    # Weight Bigrams higher than raw frequency\n",
    "                    bigram_score = self.bigram_counts[(prev_token, cand)]\n",
    "                    unigram_score = self.unigram_counts[cand]\n",
    "\n",
    "                    score = (bigram_score * 10) + unigram_score\n",
    "\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_word = cand\n",
    "\n",
    "            corrected_tokens.append(best_word)\n",
    "\n",
    "        # Reconstructing sentence\n",
    "        return \" \".join(corrected_tokens)\n",
    "\n",
    "# 1. Initialize and Train\n",
    "baseline_model = BaselineSpellCorrector()\n",
    "baseline_model.train(train_df['correct_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj0u2K7sjoff"
   },
   "source": [
    "## 3.1.1 Prediction on Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AerQSfQGjq_b"
   },
   "source": [
    "#### Prediction on a Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1765509353063,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "hYV4mLdNjsQx",
    "outputId": "c349a826-ab82-45a3-ca48-1437d0c2c955"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Baseline Model Predictions (Sample) ---\")\n",
    "for idx, row in test_df.head(5).iterrows():\n",
    "    input_text = row['incorrect_sentence']\n",
    "    ground_truth = row['correct_sentence']\n",
    "    prediction = baseline_model.correct_sentence(input_text)\n",
    "\n",
    "    print(f\"Input:    {input_text}\")\n",
    "    print(f\"Pred:     {prediction}\")\n",
    "    print(f\"Actual:   {ground_truth}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z7eLZKCjvG_"
   },
   "source": [
    "#### Prediction on a full test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62430,
     "status": "ok",
     "timestamp": 1765509433076,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "QJecyz4Ojwm9",
    "outputId": "48207b10-4eac-4153-bc11-7d53f52146b6"
   },
   "outputs": [],
   "source": [
    "if 'test_df' in locals():\n",
    "    print(\"Predicting with Baseline...\")\n",
    "    test_df['baseline_prediction'] = test_df['incorrect_sentence'].apply(baseline_model.correct_sentence)\n",
    "    print(\"Baseline predictions complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WXuY2f5j0xP"
   },
   "source": [
    "# 4.Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z0j8s53j3K3"
   },
   "source": [
    "## 4.1 Baseline Model\n",
    "#### Word-Level Accuracy, WER, CER, BLEU and Mean Noun Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210,
     "referenced_widgets": [
      "fbb1d248f8a549e9ab46f850c96adc87",
      "80a67ec996b24f389ce5d7324a2cc564",
      "c6cdadaef52d445e878416f6bb6af679",
      "d22b2c1cf69f4837abb8f6d6ec7e29c2",
      "a2744625368d4e679354d20c6587afbf",
      "a35982a99cdf480bb2fde4206ad1b190",
      "c310cfc5f81e4ad8955b6a8a0e25d8fe",
      "0d3065bacb964d8dbf951d2f33ead42b",
      "9761975554574682bc57b54966dd285e",
      "07c62a8e7ee1418282d87ca640ce3dff",
      "13f768ecc5b8434fb5095c53fb99bc1d"
     ]
    },
    "executionInfo": {
     "elapsed": 59967,
     "status": "ok",
     "timestamp": 1765509556032,
     "user": {
      "displayName": "Aryan Prajapati",
      "userId": "07150843219375122767"
     },
     "user_tz": 480
    },
    "id": "1KVWcOhVj48O",
    "outputId": "d4bd4f2d-4ed4-4a1c-e21d-739af258182a"
   },
   "outputs": [],
   "source": [
    "def calculate_word_level_accuracy():\n",
    "  print(\"Calculating Accuracy on Test Set...\")\n",
    "  total_words = 0\n",
    "  correct_words = 0\n",
    "  for idx, row in test_df.iterrows():\n",
    "      # Stripping punctuation for the metric calculation to be fair\n",
    "      pred_words = baseline_model.correct_sentence(row['incorrect_sentence']).split()\n",
    "      target_words = str(row['correct_sentence']).lower().split()\n",
    "\n",
    "      # Comparing word-for-word\n",
    "      length = min(len(pred_words), len(target_words))\n",
    "      for i in range(length):\n",
    "          if pred_words[i] == target_words[i]:\n",
    "              correct_words += 1\n",
    "\n",
    "      total_words += len(target_words)\n",
    "\n",
    "  accuracy = correct_words / total_words if total_words > 0 else 0\n",
    "  print(f\"\\n Word-Level Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    d = np.zeros((len(ref_words)+1, len(hyp_words)+1))\n",
    "    for i in range(len(ref_words)+1): d[i,0] = i\n",
    "    for j in range(len(hyp_words)+1): d[0,j] = j\n",
    "    for i in range(1, len(ref_words)+1):\n",
    "        for j in range(1, len(hyp_words)+1):\n",
    "            cost = 0 if ref_words[i-1] == hyp_words[j-1] else 1\n",
    "            d[i,j] = min(d[i-1,j]+1, d[i,j-1]+1, d[i-1,j-1]+cost)\n",
    "    return d[-1,-1] / len(ref_words) if len(ref_words)>0 else 0\n",
    "\n",
    "def evaluate_detailed(df, pred_col, target_col='correct_sentence'):\n",
    "    wer_scores = []\n",
    "    cer_scores = []\n",
    "    bleu_scores = []\n",
    "    noun_scores = []\n",
    "\n",
    "    print(f\"\\n--- Detailed Evaluation for {pred_col} ---\")\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        ref = str(row[target_col])\n",
    "        hyp = str(row[pred_col])\n",
    "\n",
    "        # WER\n",
    "        wer_scores.append(calculate_wer(ref, hyp))\n",
    "\n",
    "        # CER (Approximate using SequenceMatcher ratio)\n",
    "        cer_scores.append(1 - difflib.SequenceMatcher(None, ref, hyp).ratio())\n",
    "\n",
    "        # BLEU\n",
    "        try:\n",
    "            bleu = sentence_bleu([ref.split()], hyp.split(), weights=(1,0,0,0)) # BLEU-1\n",
    "        except:\n",
    "            bleu = 0\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "        # Noun Recall\n",
    "        if 'correct_nouns' in row and row['correct_nouns']:\n",
    "            nouns = row['correct_nouns']\n",
    "            hyp_words = set(hyp.split())\n",
    "            matches = sum(1 for n in nouns if n in hyp_words)\n",
    "            noun_scores.append(matches / len(nouns))\n",
    "        else:\n",
    "            noun_scores.append(1.0) # No nouns to miss\n",
    "\n",
    "    print(f\"Mean WER: {np.mean(wer_scores):.4f} (Lower is better)\")\n",
    "    print(f\"Mean CER: {np.mean(cer_scores):.4f} (Lower is better)\")\n",
    "    print(f\"Mean BLEU-1: {np.mean(bleu_scores):.4f} (Higher is better)\")\n",
    "    print(f\"Mean Noun Recall: {np.mean(noun_scores):.4f} (Higher is better)\")\n",
    "\n",
    "if 'baseline_prediction' in test_df.columns:\n",
    "    calculate_word_level_accuracy()\n",
    "    evaluate_detailed(test_df, 'baseline_prediction')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6QV1912xb8tt86TdWuGf+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07c62a8e7ee1418282d87ca640ce3dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d3065bacb964d8dbf951d2f33ead42b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13f768ecc5b8434fb5095c53fb99bc1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80a67ec996b24f389ce5d7324a2cc564": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a35982a99cdf480bb2fde4206ad1b190",
      "placeholder": "​",
      "style": "IPY_MODEL_c310cfc5f81e4ad8955b6a8a0e25d8fe",
      "value": "100%"
     }
    },
    "9761975554574682bc57b54966dd285e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2744625368d4e679354d20c6587afbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a35982a99cdf480bb2fde4206ad1b190": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c310cfc5f81e4ad8955b6a8a0e25d8fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6cdadaef52d445e878416f6bb6af679": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d3065bacb964d8dbf951d2f33ead42b",
      "max": 1497,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9761975554574682bc57b54966dd285e",
      "value": 1497
     }
    },
    "d22b2c1cf69f4837abb8f6d6ec7e29c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07c62a8e7ee1418282d87ca640ce3dff",
      "placeholder": "​",
      "style": "IPY_MODEL_13f768ecc5b8434fb5095c53fb99bc1d",
      "value": " 1497/1497 [00:01&lt;00:00, 786.43it/s]"
     }
    },
    "fbb1d248f8a549e9ab46f850c96adc87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_80a67ec996b24f389ce5d7324a2cc564",
       "IPY_MODEL_c6cdadaef52d445e878416f6bb6af679",
       "IPY_MODEL_d22b2c1cf69f4837abb8f6d6ec7e29c2"
      ],
      "layout": "IPY_MODEL_a2744625368d4e679354d20c6587afbf"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
